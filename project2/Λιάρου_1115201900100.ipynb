{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Λιάρου_1115201900100.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Εργασία 2 <br> Μηχανική Μάθηση  <br> Ελένη Λιάρου <br> 1115201900100**"
      ],
      "metadata": {
        "id": "-wuV23Dd5q0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ερώτημα 1: Logistic regression and overfitting<br>**"
      ],
      "metadata": {
        "id": "He6e-jpj6CaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(z):\n",
        "    sig = 1/(1+math.e**(-z))\n",
        "    return sig\n",
        "\n",
        "\n",
        "for w in [1, 5, 100]:\n",
        "  print(\"plotting for w = \" , w)\n",
        "  x = np.linspace(-10, 10)\n",
        "  z = np.dot(w,x)\n",
        "  s = sigmoid(z)\n",
        "  plt.plot(x ,s)\n",
        "  plt.xlabel('x axis')\n",
        "  plt.ylabel('sigmoid for w = ')\n",
        "  plt.show()\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "D_Dor73J8lGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting για μεγάλες τιμές του w <br>**\n",
        "Παρατηρούμε ότι όσο μεγαλώνει η τιμή του w η γραφική της sigmoid γίνεται όλο και πιο απότομη, με αποτέλεσμα να είναι πολύ πιο δύσκολο να εφαρμόσουμε gradient descent, αφού οι παράγωγοι είναι κυρίως κοντά στο 0, εκτός από το σημείο που τέμνει τον άξονα y και μετά, όπου οι τιμές των παραγώγων γίνονται πολύ μεγάλες. Το overfitting συνήθως οφείλεται στις ασυμπωτικές προβλεψεις, στο perfect separation και στο curse of dimensionality<br> \n",
        "**Κανονικοποίηση του w <br>**\n",
        "Η κανονικοποίηση της παραμέτρου w θα βοηθήσει αρκετά στην αντιμετώπιση του overfitting. Η κανονικοποίηση βασίζεται στο ότι μικρότερα βάρη οδηγούν σε απλούστερα μοντέλα και έτσι αποφεύγουμε το overfitting. Για να έχουμε έναν μικρότερο πίνακα βάρους απλά πρέπει στην συνάρτηση κόστους να προσθέσουμε έναν επιπλέον όρο, τον όρο κανονικοποίησης, ο οποίος ποινικοποιεί (penalize) την υπέρμετρη προσαρμογή του μοντέλου. Επομένως έχουμε $cost function = loss + λ *\\sum{||w||^2}$, όπου το λ ισούται με το άθροισμα των τετραγώνων όλων των βαρών. Αυτή η τεχνική κανονικοποίησης μειώνει τα βάρη χωρίς να τα μηδενίζει και είναι προτιμότερο να την χρησιμοποιούμε όταν όλα τα δεδομένα εισόδου επηρεάζουν την έξοδο και τα βάρη έχουν περίπου το ίδιο μέγεθος. Όταν τα βάρη είναι πολύ μεγάλα οδηγούν σε μεγάλη διακύμανση εξόδου και ασταθή μοντέλα. Μέσω της παραμέτρου λ ελέγχουμε πόσο πολύ κάνουμε fit τα δεδομένα και σχετίζεται με την ισχύ της ποινής που προστίθεται κάθε φορά, αν αυξήσουμε το λ, αυξάνεται και η επιρροή της κανονικοποίησης τείνοντας τους συντελεστές στο 0, ενώ αν μείσωσουμε το λ είναι σαν να έχουμε κανονική γραμμική παλινδρόμηση ."
      ],
      "metadata": {
        "id": "XV2qR82xVKHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ερώτημα 2: Ridge regression**"
      ],
      "metadata": {
        "id": "xDs5lZTx7YVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Από matrix cookbook γνωρίζουμε ότι $\\frac{\\partial (Y-β^ΤΧ)^Τ(Y-β^ΤΧ)}{\\partial β} = -2X^T(Y-β^ΤΧ)$ και $\\frac{\\partial λβ^Τβ}{\\partial b} = 2λβ$<br>\n",
        "Η εξίσωση της linear regression είναι: $(y-Xβ)^Τ(y-Xβ) =0 \\Rightarrow X^TXβ=X^Ty$ <br>\n",
        "Γνωρίζουμε ότι $λ\\ge 0$ άρα υπάρχει θετική ρίζα ν ώστε λ = $ν^2$.\n",
        "Ορίζουμε έναν νέο επαυξημένο πίνακα με γραμμές που αντιστοιχούν σε ν φορές τον $KxK$ μοναδιαίο πίνακα Ι, $Χ_* = \\binom{X}{νI}$. Ορίζουμε και έναν επαυξημένο διάνυσμα $y_* = \\binom{y}{0_{Kx1}}$.  <br>Επομένως θεωρούμε την εξής συνάρτηση $(y_* - X_*β)^T(y_*-X_*β)$. Επειδή αυτό είναι ο πολλαπλασιασμός ενός διανύσματος με τον ανάστροφό του το αποτελέσμα ισούται με το άθροισμα των τετραγώνων των τιμών του διανύσματος. Εξαιτίας τηε επάυξησης που εφαρμόσαμε θα υπάρχουν K επιπλεόν $(0-νβ_i)^2 = ν^2β_i^2 = λβ_i^2 = λβ^Τβ$ όρους απότι αν είχαμε τα αρχικά Χ και y. $(y-Xβ)^Τ(y-Xβ) + λβ^Τβ = (y_*-X_*β)^T(y_*-X_*β)$ <br>\n",
        "Το β επειδή ελαχιστοποιεί την αρχική εξίσωση θα ελαχιστοποιεί και την καινούργια εξίσωση. Από την γραμμική άλγεβρα γνωρίζουμε ότι υπάρχει β τέτοιο ώστε $(y_*-X_*β)^Τ(y_*-X_*β) =0$. Ο τελικός πίνακας δυστυχώς έχει μεγάλο rank. Άρα υπάρχει β τέτοιο ώστε $(y-Xβ)^Τ(y-Χβ) + λβ^Τβ =0$. Αυτή η επιλογή του β θα μειώσει σίγουρα την εξίσωση αφού θα την κάνει ίση με 0. Χρησιμοποιώντας την εξίωση της linear regression προκύπτει ότι $(Χ^ΤΧ+λI)β = Χ^Τy ⇒ β = (X^TX+λI)^{-1}X^Ty$ <br>\n",
        "Άλλος ένας τρόπος είναι ο εξής: <br>\n",
        "$Ridge = (y-Xβ)^Τ(y-Xβ)+λβ^Τβ = y^Ty-β^TX^Ty - y^TXβ + β^Tx^TXβ + λβ^Τβ = y^Ty-2β^TX^Ty + β^Τ(Χ^ΤΧ+λI)β$. $\\frac{\\partial Ridge}{\\partial β} = -2X^Ty+2(X^TX+λI)β=0 \\Rightarrow (X^ΤX+λI)β = X^Ty ⇒ β = (X^TX+λI)^{-1}X^Ty$\n",
        "\n",
        "**Μοναδικότητα λύσης όταν λ>0 <br>**\n",
        "όταν χρησιμοποιούμαι ridge regression η προσθήκη του όρου λΙ στον sample covariance matrix (Χ^ΤΧ) διασφαλίζει ότι όλες οι ιδιοτιμές θα είναι αυστηρά μεγαλύτερες από 0, άρα δεν θα είναι αντιστρέψιμος και επομένως η λύση γίνεται μοναδική. "
      ],
      "metadata": {
        "id": "cYwQldcBojtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ερώτημα 3: Face recognition**"
      ],
      "metadata": {
        "id": "TBuBD5TLYMyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ζητούμενο 1: υλοποίηση συνάρτησης loadImages(path, set_number)"
      ],
      "metadata": {
        "id": "P8aDQwJaYXC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#some neseccary imports, have to compile before executing following code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from PIL import Image\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "\n",
        "\n",
        "path = '/content/gdrive/MyDrive/faces'"
      ],
      "metadata": {
        "id": "TlEN-tgGwEFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f656b17-bccf-46f9-d1d5-f0e5fa466547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ερώτημα 3.1**<br>\n",
        "ως preproccesing έχω εφαρμόσει την αφαίρεση και την διαίρεση με το mean και το std της εικόνας αντίστοιχα  "
      ],
      "metadata": {
        "id": "MJ5PaMsRwM_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadImages(path, set_number):\n",
        "  images = []\n",
        "  labels = []\n",
        "\n",
        "  if(set_number == \"Set_1\"):\n",
        "      min = 1\n",
        "      max = 7\n",
        "  elif (set_number == \"Set_2\"):\n",
        "      min = 8\n",
        "      max = 19\n",
        "  elif (set_number == \"Set_3\"):\n",
        "      min = 20\n",
        "      max = 31\n",
        "  elif (set_number == \"Set_4\"):\n",
        "      min = 32\n",
        "      max = 45\n",
        "  elif (set_number == \"Set_5\"):\n",
        "      min = 46\n",
        "      max = 64\n",
        "\n",
        "  for picname in os.listdir(path):\n",
        "    if(int(picname[9]+picname[10])< min or int(picname[9]+picname[10])>max):\n",
        "      continue\n",
        "    img = Image.open(os.path.join(path, picname))\n",
        "    img = img.resize((50,50))\n",
        "    img = np.asarray(img)\n",
        "    img.reshape(-1)\n",
        "    img = img.flatten()\n",
        "    mean = img.mean()\n",
        "    std = img.std()\n",
        "    img = img-mean\n",
        "    img = img/std\n",
        "    images.append(img)\n",
        "    if picname[6] == '0':\n",
        "      tmp = picname[7]\n",
        "    else:\n",
        "      tmp = picname[6] + picname[7]\n",
        "    labels.append(int(tmp))\n",
        "  images = np.asarray(images)\n",
        "  #mean = images.mean()\n",
        "  #std = images.std()\n",
        "  #images = images-mean\n",
        "  #images = images/std\n",
        "  return images, labels"
      ],
      "metadata": {
        "id": "bEjttyZ6YXeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ερώτημα 3.2**<br>\n",
        "κάνω preproccesing σε όλα τα set και στην PCA εφαρμόζω whiten"
      ],
      "metadata": {
        "id": "a4YAB1dDwSNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for j in [9, 30]:\n",
        "  images1, labels1 = loadImages(path, \"Set_1\")\n",
        "  images2, labels2 = loadImages(path, \"Set_2\")\n",
        "  images3, labels3 = loadImages(path, \"Set_3\")\n",
        "  images4, labels4 = loadImages(path, \"Set_4\")\n",
        "  images5, labels5 = loadImages(path, \"Set_5\")\n",
        "\n",
        "  print(\"Calculating accuracy for d = \", j)\n",
        "  pca = PCA(n_components=j, whiten=True).fit(images1)\n",
        "  images1 = pca.transform(images1)\n",
        "  images2 = pca.transform(images2)\n",
        "  images3 = pca.transform(images3)\n",
        "  images4 = pca.transform(images4)\n",
        "  images5 = pca.transform(images5)\n",
        "\n",
        "# Make an instance of knn model\n",
        "  knn = KNeighborsClassifier(n_neighbors=1)\n",
        "  knn.fit(images1, labels1)\n",
        "\n",
        "  score1 = knn.score(images1, labels1)\n",
        "  print(\"accuracy for Set_1\", score1)\n",
        "\n",
        "  score2 = knn.score(images2, labels2)\n",
        "  print(\"accuracy for Set_2\", score2)\n",
        "\n",
        "  score3 = knn.score(images3, labels3)\n",
        "  print(\"accuracy for Set_3\", score3)\n",
        "\n",
        "  score4 = knn.score(images4, labels4)\n",
        "  print(\"accuracy for Set_4\", score4)\n",
        "\n",
        "  score5 = knn.score(images5, labels5)\n",
        "  print(\"accuracy for Set_5\", score5)\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KN3wJqn3wUp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ερώτημα 3.3**"
      ],
      "metadata": {
        "id": "51qlfwEFvsqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images1, labels1 = loadImages(path, \"Set_1\")\n",
        "n_components = 9 ## how may componets to compute\n",
        "\n",
        "pca = PCA(n_components=9, whiten=True).fit(images1)\n",
        "components = pca.transform(images1)\n",
        "plt.figure(figsize=(25, 25))\n",
        "for i in range(0, 9, 1):\n",
        "  plt.subplot(1, 9, i+1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(pca.components_[i].reshape(50, 50), cmap='bone')\n"
      ],
      "metadata": {
        "id": "1GPk1MITvznU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ερώτημα 3.4**"
      ],
      "metadata": {
        "id": "fP_LwxoOyZUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "images1, labels1 = loadImages(path, \"Set_1\")\n",
        "images2, labels2 = loadImages(path, \"Set_2\")\n",
        "images3, labels3 = loadImages(path, \"Set_3\")\n",
        "images4, labels4 = loadImages(path, \"Set_4\")\n",
        "images5, labels5 = loadImages(path, \"Set_5\")\n",
        "\n",
        "size1 = images1.shape[0]\n",
        "size2 = images2.shape[0]\n",
        "size3 = images3.shape[0]\n",
        "size4 = images4.shape[0]\n",
        "size5 = images5.shape[0]\n",
        "\n",
        "tmp1 = random.randint(0, size1)\n",
        "tmp2 = random.randint(0, size2)\n",
        "tmp3 = random.randint(0, size3)\n",
        "tmp4 = random.randint(0, size4)\n",
        "tmp5 = random.randint(0, size5)\n",
        "\n",
        "print(\"Reconstructing images with d = 9 and d = 30\")\n",
        "print(\"first column is original image, second column is reconstruction for d=9 and third column is reconstruction for d=30\")\n",
        "pca = PCA(n_components=9, whiten=True).fit(images1)\n",
        "images1_ = pca.transform(images1)\n",
        "images2_ = pca.transform(images2)\n",
        "images3_ = pca.transform(images3)\n",
        "images4_ = pca.transform(images4)\n",
        "images5_ = pca.transform(images5)\n",
        "\n",
        "\n",
        "temp1 = pca.inverse_transform(images1_)\n",
        "temp2 = pca.inverse_transform(images2_)\n",
        "temp3 = pca.inverse_transform(images3_)\n",
        "temp4 = pca.inverse_transform(images4_)\n",
        "temp5 = pca.inverse_transform(images5_)\n",
        "\n",
        "pca = PCA(n_components=9, whiten=True).fit(images1)\n",
        "images1__ = pca.transform(images1)\n",
        "images2__ = pca.transform(images2)\n",
        "images3__ = pca.transform(images3)\n",
        "images4__ = pca.transform(images4)\n",
        "images5__ = pca.transform(images5)\n",
        "\n",
        "\n",
        "temp1_ = pca.inverse_transform(images1__)\n",
        "temp2_ = pca.inverse_transform(images2__)\n",
        "temp3_ = pca.inverse_transform(images3__)\n",
        "temp4_ = pca.inverse_transform(images4__)\n",
        "temp5_ = pca.inverse_transform(images5__)\n",
        "\n",
        "  \n",
        "f, p = plt.subplots(5,3)\n",
        "\n",
        "p[0,0].axis('off')\n",
        "p[0,0].imshow(images1[tmp1].reshape(50, 50), cmap='bone')\n",
        "p[0,1].axis('off')\n",
        "p[0,1].imshow(temp1[tmp1].reshape(50, 50), cmap='bone')\n",
        "p[0,2].axis('off')\n",
        "p[0,2].imshow(temp1_[tmp1].reshape(50, 50), cmap='bone')\n",
        "  \n",
        "p[1,0].axis('off')\n",
        "p[1,0].imshow(images2[tmp2].reshape(50, 50), cmap='bone')\n",
        "p[1,1].axis('off')\n",
        "p[1,1].imshow(temp2[tmp2].reshape(50, 50), cmap='bone')\n",
        "p[1,2].axis('off')\n",
        "p[1,2].imshow(temp2_[tmp2].reshape(50, 50), cmap='bone')\n",
        "\n",
        "p[2,0].axis('off')\n",
        "p[2,0].imshow(images3[tmp3].reshape(50, 50), cmap='bone')\n",
        "p[2,1].axis('off')\n",
        "p[2,1].imshow(temp3[tmp3].reshape(50, 50), cmap='bone')\n",
        "p[2,2].axis('off')\n",
        "p[2,2].imshow(temp3_[tmp3].reshape(50, 50), cmap='bone')\n",
        "\n",
        "p[3,0].axis('off')\n",
        "p[3,0].imshow(images4[tmp4].reshape(50, 50), cmap='bone')\n",
        "p[3,1].axis('off')\n",
        "p[3,1].imshow(temp4[tmp4].reshape(50, 50), cmap='bone')\n",
        "p[3,2].axis('off')\n",
        "p[3,2].imshow(temp4_[tmp4].reshape(50, 50), cmap='bone')\n",
        "\n",
        "p[4,0].axis('off')\n",
        "p[4,0].imshow(images5[tmp5].reshape(50, 50), cmap='bone')\n",
        "p[4,1].axis('off')\n",
        "p[4,1].imshow(temp5[tmp5].reshape(50, 50), cmap='bone')\n",
        "p[4,2].axis('off')\n",
        "p[4,2].imshow(temp5_[tmp5].reshape(50, 50), cmap='bone')\n",
        "\n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "oqpqMawqyb_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ερώτημα 3.5**"
      ],
      "metadata": {
        "id": "nFa8G-Q_ycYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images1, labels1 = loadImages(path, \"Set_1\")\n",
        "U, S, V = np.linalg.svd(images1)\n",
        "#reconst_img_9 = np.matrix(U[:, :9]) * np.diag(S[:9]) * np.matrix(V[:9, :])\n",
        "print(\"Showing singular vectors\\n\")\n",
        "plt.figure(figsize=(25, 25))\n",
        "for i in range(0, 9, 1):\n",
        "  plt.subplot(1, 9, i+1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(V[i].reshape(50, 50), cmap='bone')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Xib96zgczY1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Παρατηρήσεις:**<br>\n",
        "\n",
        "\n",
        "1.   ερώτημα 3.2<br>\n",
        "Η μέθοδος μπορεί να γενικευτεί σίγουρα για τα 3 πρώτα set αφού ακόμα και για μικρό d πάιρνουμε υψηλό accuracy, για το set5 όπου οι φωτογραφίες είναι αρκετά σκοτεινές ακόμα και με whiten δεν έχουμε υψηλό ποσοστό ακριβείας. Ενώ στο set4 για μεγαλύτερο d παρατηρούμε αισθητή βελτίωση στο score. Είχα δοκιμάσει και κανονικοποίηση χρησιμοποιώντας το mean και το std όλου του set και όχι κάθε εικόνας ξεχωριστά όμως το accuracy ήταν πιο χαμηλό. \n",
        "2.   ερώτημα 3.4<br>\n",
        "Παρατηρώ ότι όσο πιο φωτεινή είναι η αρχική εικόνα τόσο πιο ακριβής είναι η ανακατασκευή της. Επίσης για μεγαλύτερο d η ανακατασκευή είναι λίγο πιο κοντά στην αρχική εικόνα. Για πιο σκοτεινές εικόνες η ανακατεσκευή είναι αρκετά θολή σε σύγκριση για πιο φωτεινές. \n",
        "3.   ερώτημα 3.5<br>\n",
        "Τα singular vectors που απεικόνισα σε αυτό το ερώτημα με την SVD, διαφέρουν αρκετά σε σχέση με τα eigenvectors του PCA που βρήκα στο ερώτημα 3. Τα eigenvectors με τα singular values ενός πίνακα Τ είναι ίσα όταν ο πίνακας Τ είναι κανονικός $Τ^*Τ=ΤΤ^*$, στην περίπτωση που εξετάζουμε όμως ο πίνακας των εικόνων δεν είναι κανονικός άρα οι 2 μέθοδοι φτιάχνουν διαφορετικές εικόνες. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RwLuSQjxzcct"
      }
    }
  ]
}